%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Chapter 1 : Introduction
%%
%%      * Should give an intro/overview of the problem and the context in which it arises
%%      * Then, it should emphasize the problem (problem statement)
%%      * Then the objectives of the proposed research topic and problem
%%      * And finally the organization/structure of the document
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:intro}

\input{./chapters/chapter_1/chapter_1_figs}

Recent advances in the field of Deep Reinforcement Learning (DRL) have achieved impressive
results in various complex tasks, like being able to play Atari games at a super human level \citep{DQNAtari}, 
and beating the world's Go champion \citep{AlphaGo}, just to name a few. These represent discrete 
action spaces problems, in which the action space can be defined by a finite-size set of values. 
More recently, this approach has also been applied to continuous actions spaces, i.e. in continuous 
control tasks. This has given good results in various locomotion benchmarks, like in 
\citeauthor{DeepmindEmergenceLocomotion} and \citeauthor{DeepMimic}, shown in Figure @FIG-1.1.

\figDrlLocomotionMotivation

This approach is very promising because it allows an agent to learn an appropiate controller for a given task after training in
the simulated environment, which reduces the need to implement highly sophisticated control pipelines. Currenttly available 
training environments have been presented in various articles, and we will talk more about these in Chapter 3.
Some of these currently available training environments are shown in Figure @FIG-1.2.

%% and compared to the ones mentioned previously (from Figure @FIG-1.1)
%%they are much simpler, consisting mostly of classic control problems and some locomotion tasks in flat terrains.

The ultimate goal of applying these techniques is to be able to develop complex behaviours, similar
to those shown in nature by animals. However, to develop and test this complex behaviours there is a need to \textbf{create
a wide set of rich and complex environments}, which is not provided by currently available locomotion benchmarks.

\figDrlBenchmarks

% @TAG: Refer to content above and "formalize" a bit more
\section{Problem Statement}
\label{sec:problem}

As explained previously, currently there are no robot locomotion benchmarks that allow
to \textbf{create diverse learning environments}, which limits the range of behaviours that can be learned
from scratch using Deep Reinforcement Learning techniques. Besides, there are some other issues 
that arise in the context of learning locomotion behaviours:

\begin{itemize}
	\item \textbf{Exploitation} of the dynamics of the simulated environments: 
			Because of the nature of the objective the agents try to optimize for, they usually
			end up developing behavious that cheat by exploiting some specific aspects of the environment
			and getting stuck sub-optimal behaviours, e.g. walking in very unnatural way in walking tasks.
			This is specially true when designing a reward function. The reward is the signal
			that the agent gets to improve its behaviours, so poorly engineered reward functions could lead to sub-optimal and even bad performance.
	\item \textbf{Inability to transfer} learned  policies into the real world:
			In general, policies learned in simulation do not transfer directly to a real world robotics
			platform. This is because of discrepancies between the dynamics of the simulation and
			the dynamics of the real world, which is usually called the \textbf{reality gap}.
			This issue is also a direct consequence of the previous one. As the agent exploits the 
			dynamics of the simulation, it will overfit to the given environment, and will 
			not generalize when transferred to the real world.
\end{itemize}

Some approaches used to deal with this issues are related to controlling the learning 
environments itself, like in \citeauthor{GoogleBrainSim2Real} and \citeauthor{OpenAISim2real} (Figure @FIG-1.3). 
These try to account for unmodeled dynamics and generalization by using randomization
over the simulated environment and other techniques to force the learning agent not to 
overfit to a specific behaviour that only exploits some of the dynamics of the environment.

Then, one approach that can be used to solve these issues is to have a framework
that provides researchers with the tools required to create these diverse environments
and have full control over them. Furthermose, such a framework could be used to develop
environments in a progressive way via a \textbf{curricula} (proposed in @CITE). Based on
all this, we formulate the following hypothesis:

\begin{hypothesis}
	By having control of the learning environment we can control the learning procedure
	of an agent in such a way that it can develop complex and robust behaviours to solve
	the tasks presented, and also generalize to other environments.
\end{hypothesis}

\figEnvManipSimToreal

\figEnvironmentsProposalFromTo

% @TAG: Refer to initial content and "emphazise" the objectives
\section{Objectives}
\label{sec:objectives}

\subsection*{General Objective}
Make a framework to build diverse learning environments for locomotion tasks (as shown in Figure @FIG-1.4), 
and evaluate the performance and generalization capabilities of current state of the art Deep 
Reinforcement Learning algorithms.

\subsection*{Specific Objectives}
\begin{itemize}
 \item Implement software tools on top of current Reinforcement Learning APIs and physics engines 
       to create complex RL tasks for continuous control.
 \item Evaluate the performance of state of the art Deep Reinforcement Learning algorithms in these environments.
\end{itemize}

\section{Organization}
\label{sec:organization}

The structure of this proposal is as follows :

\begin{itemize}
	\item \textbf{Chapter 2 - Related background}: we present the necessary background and
		  terminology used throught this document.
	\item \textbf{Chapter 3 - Related works}: we discuss the currently available benchmarks by
		  analyzing the provided functionality and use cases. We also discuss some
		  state of the art algorithms used with these benchmarks, which will be
		  implemented and evaluated with the proposed framework.
	\item \textbf{Chapter 4 - Proposal in depth}: we discuss more details of the proposed framework,
		  including: architecture of the framework, core features to be implemented, etc.
	\item \textbf{Chapter 5 - Current progress}: we discuss the current state of the implementation
		  of the proposed framework.
\end{itemize}