%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Chapter 3 : Related works
%%
%%      * Should give an overview of what the big dogs are saying in the field
%%
%%  BASIC STRUCTURE :
%%
%%      a. Current benchmarks for robot locomotion
%%          * DeepMind's ControlSuite
%%          * OpenAI's mujoco-py
%%          * Berkeley's garage and rllab
%%          * Stanford's robosuite
%%          * NVIDIA Isaac + FleX
%%          * Eth - raisim simulator?
%%          * UBC's terrainRlSim
%%          * Unity's ml-agents (talk about the marathon envs)
%%
%%      b. DRL algorithms for robot locomotion
%%          * Benchmarking drl for continuous ccontrol (2016?)
%%          * Trust region policy optimization
%%          * Proximal policy optimization
%%          * DeepTerrainRL
%%          * DeepLoco
%%          * DeepMimic
%%          * Emergence of locomotion in rich and complex environments
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Works}
\label{ch:relatedWorks}

\input{./chapters/chapter_3/chapter_3_figs}

In this chapter we will give an overview of the state of the art in robot locomotion using 
Deep Reinforcement Learning. We discuss some currently avaiable benchmarks for these tasks,
giving details that will help compare these current benchmarks with the proposed framework, which
will be discussed in more detail in chapter 4. We also discuss current state of the art algorithms
used to solve these tasks, which will be used as baselines for the experiments we will propose as part
of the evaluation procedure.

\section{Robot locomotion benchmarks}

\subsection{Deepmind ControlSuite}

This set of benchmarks was first introduced in \cite{Controlsuite}, and consists 
of various locomotion tasks (shown in Figure @FIG-3.1), with various types of agents 
to select from. The models available consist of xml files in the mujoco format (mjcf) 
that are used as the blueprints to instantiate an agent. The tasks consist of the specific
setups that the agents must solve, like walking, standing, running, etc.

\figBenchmarkControlSuite

As described in their \href{https://arxiv.org/pdf/1801.00690.pdf}{technical report},
some of the key features of this benchmark include:

\begin{itemize}
    \item \textbf{Full state observations}: the observations provided to the agent
          are sufficient to recover the full state of the environment. These observations
          include positions and velocities (of bodies and joints), touch sensor readings,
          and some other task specific measurements. Using this information, the full state
          of the environment can be recovered due to the simple nature of the environment 
          (most locomotion tasks in the benchmark are flat terrain tasks, and no information
          is hidden from the agents).

    \item \textbf{Normalized actions}: the actions exposed to the user have been normalized 
          in the range $\left[-1,1\right]$. These are mapped to actuators that represent
          torques applied to joints (torque actuation model).

    \item \textbf{Normalized rewards}: the rewards are set to the range $\left[ 0, 1 \right]$, 
          and can be smooth (whole range) or sparse (just the values $\left\{0,1\right\}$).

    \item \textbf{MuJoCo backend}: MuJoCo is the underlying physics backend, and it is
          integrated into the suite by using Python bindings generated using 
          \href{https://github.com/deepmind/dm_control/blob/master/dm_control/autowrap/autowrap.py}{ctypes}.
\end{itemize}

\subsection{OpenAI Gym}

This set of benchmarks consists of two separate options depending on the physics 
backend (MuJoCo and Bullet). These two options are Python Wrappers for the supported 
physics backends and are used as low level functionality for the high level API provided by Gym \citep{Gym}.
The options used as wrappers for the two physics backends available are the following:

\begin{itemize}
    \item \textbf{Mujoco-py} : A Python wrapper for MuJoCo, very similar to ControlSuite. 
          The available tasks are similar to the ones in ControlSuite, which are shown 
          in Figure @FIG-3.2.

        \figBenchmarkOpenAIGymMujoco

    \item \textbf{RoboSchool}: An implementation that uses PyBullet, which exposes 
          the Bullet physics engine through a Python API. The environments exposed 
          are a bit different to the previous suite, and are shown in Figure @FIG-3.3. 

        \figBenchmarkOpenAIGymRoboschool

\end{itemize}

The RL API exposed by Gym is similar to the one exposed by ControlSuite. 
In each step taken in the environment, the API returns the observation, 
reward, and some extra observations. More information can be found in its 
technical report \citep{Gym}, and in its \href{https://github.com/openai/gym}{repository}.

\subsection{Rllab and Garage}

Rllab is a set of benchmarks similar to the previous two. It implements its own 
Python wrapper for MuJoCo, and builds its own Reinforcement Learning API on top 
of that wrapper. It provides a set of tested baselines of various Deep Reinforcement 
Learning algorithms, which were compared in \cite{Rllab}. 

The environments provided by Rllab are grouped in three categories: \textbf{classic} 
(Figure @FIG-3.4), \textbf{locomotion} (Figure @FIG-3.5) and \textbf{hierarchical} (Figure @FIG-3.6).
It is compatible with Gym, by means of a wrapper on top of its environments, 
but as they explain in their documentation \href{https://rllab.readthedocs.io/en/latest/user/gym_integration.html}{website} 
this is a very different API.

\figBenchmarkRllabClassic

\figBenchmarkRllabLocomotion

\figBenchmarkRllabHierarchical

Garage is the next version of the Rllab suite, and is very similar in architecture 
to Rllab, with support for more environments and baselines. To this date the number 
of environments is similar to the ones in Rllab, but is being actively supported, 
compared to Rllab. More information can be found in its \href{https://github.com/rlworkgroup/garage}{repo}.

\subsection{Robosuite}
 
@TODO

\subsection{Gpu-Accelerated simulator}

@TODO

\subsection{Robot simulator from Eth}

@TODO

\subsection{Unity ML-Agents}

This last suite \citep{unity-ml-agents} is a recent one, and consists of some ports of the previously mentioned environments, as well
as many more new environments that are built on top of the \citeauthor{unity} game engine. These allows to easily create new environments
for RL agents because of all the tools provided by the Unity3d engine.

One of the key differences is the type of physics engine used, which in this case is the \citeauthor{physX} (a real time physics engine
developed by NVIDIA for games). Compared with MuJoCo, this is less optimized for multibody dynamics. Instead, it is optimized to simulate
lots of objects. Another key difference is the RL API, which has the architecture shown in figure 2.8.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=3.5in]{./chapters/imgs/img_unity_mlagents_api_overview.png}
    \caption[unity ml agents overview]{Overview of the Unity ML-Agents architecture. Extracted from \citet{unity-ml-agents}}
    \label{fig:unity-ml-agents-overview}
\end{figure}

The previous architecture is exposed to the user through the API, and the basic workflow when using
this suite is shown in figure 2.9.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=3.5in]{./chapters/imgs/img_unity_mlagents_workflow.png}
    \caption[unity ml agents workflow]{Base workflow used in Unity ML-Agents. Extracted from \citet{unity-ml-agents}}
    \label{fig:unity-ml-agents-workflow}
\end{figure}