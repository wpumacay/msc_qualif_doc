%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Algorithms for chapter 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\algVanillaPolicyGradients}{
\begin{algorithm}

\caption{Vanilla Policy Gradients algorithm}
\label{alg:vanilla_policy_gradients}

    \begin{algorithmic}[1]
    \REQUIRE Initial policy parameters $\theta_{0}$

    \FOR{$k=1,2,\hdots $}
        \STATE Collect trajectories using policy $\pi_{\theta_{k}}$
        \STATE Compute advantage estimates $\hat A_{t}$
        \STATE Compute policy gradient estimate as
                \begin{equation}
                    \hat g \approx \frac{1}{N} 
                         \sum_{i=1}^{N} \sum_{t=0}^{T^{(i)}} 
                             \nabla_{\theta} \log \pi_{\theta} 
                                     ( a_{t}^{(i)} | s_{t}^{(i)} )
                                        \vert_{\theta = \theta_{k}} \hat A_{t}
                \end{equation}
        \STATE Apply policy update using SGD as
                \begin{equation}
                    \theta_{k+1} = \theta_{k} + \alpha \hat g
                \end{equation}
               or using another gradient descent algorithm.
    \ENDFOR

    \end{algorithmic}

\end{algorithm}
}